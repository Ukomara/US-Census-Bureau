Project Report

Introduction:
The ability to accurately predict the income level of an individual can be a valuable tool for businesses looking to target specific demographics with tailored marketing strategies. In this project, we aim to develop a classification model that can predict whether an individual's income level is more or less than $50,000.
To achieve this, we will be using a dataset containing weighted census data extracted from the 1994 and 1995 Current Population Surveys conducted by the U.S. Census Bureau. This dataset includes 40 demographic and employment-related variables, as well as a weight and label for each observation indicating whether an individual's income is above or below the $50k threshold.
In addition to developing a classification model, we will also be preparing a segmentation model for a retail business. The objective of this segmentation model is to identify groups of customers who share similar characteristics, so that the business can better understand their target audience for targeted advertising, pricing strategies, product selection, customer service, and customer engagement.
By developing these models, we hope to provide valuable insights for businesses looking to optimize their marketing efforts and better understand their customers.

Data Exploration and Pre-processing:
The objective of this project is to develop a classification model that predicts the income level of individuals as either more or less than $50,000, and to segment customers based on their demographic and employment-related features. To achieve this, a comprehensive exploratory analysis was conducted, including an examination of variable distributions, identification of potential outliers, missing values, and garbage values.
The analysis revealed that 93.7% of the individuals in the data set earn an income of less than $50,000, indicating an imbalanced dataset. To address this, potential explanations for the skewness were considered, such as sampling bias, and the possibility of collecting additional data from different sources or broadening the range of incomes considered was explored. Counts of each unique value or range of the columns were observed, and some unique values of features were imputed using other columns to address the 870 missing values.
Dealing with garbage values, columns such as migration code-change in MSA, migration code-change in reg, migration code-move within reg, and migration prev res in sunbelt were identified as providing no useful information and were thus dropped. The remaining garbage values, such as country of birth father, country of birth mother, country of birth self, and state of previous residence were replaced with "Other/Not known" to address the issue. Additionally, the "Not in universe" values were identified in several columns, indicating missing or incomplete data. The columns with more than 90% of values as "Not in universe" were filtered and reduced to minimize the impact of missing values.
Different plots were created to identify patterns among the variables and with the target variable. Label encoding was used to convert categorical variables into numerical ones for
implementing the models. A heatmap was visualized to identify high correlations among the variables. Finally, it was decided to drop the wage per hour details to avoid potentially revealing an individual's income or financial situation. Overall, this exploratory analysis provides valuable insights for preparing a successful segmentation model and developing a classification model for predicting income levels.

Interesting Insights:
- Most individuals who have above 50,000 income are older than 20 years
- Individuals between 30 to 60 years paticularly are mostly having income more than 50,000
- The income information for individuals after 40 years mark are gradually reducing
- The majority of the values in the 'wage per hour' column are 0, since both the median (50th percentile) and the third quartile (75th percentile) are 0
- Fascinatingly, the majority of individuals with incomes of 50,000 or more have wages per hour equal to zero
- This could indicate that they have an alternative means of income to employment or that the data is flawed
- Majority of the individuals are either Never married or Married-spouse present
- Among those with an income of more than 50,000 , 'married-civilian spouse present' is the most common marital status
- For individuals earning 50,000 or less, the majority are either 'children' or 'not in universe', indicating that a significant proportion of this population might not be a part of the workforce or not old enough to work
- For individuals earning more than 50,000, 'manufacturing-durable goods' is the most common industry
- Executive admin and managerial has the most individuals income more than $50,000 in major occupation code
- The median, 25th percentile, and 75th percentile values in 'capital gains', 'capital losses', 'dividends from stocks' columns are all zero, as can be seen, indicating that the majority of the populace does not have any capital gains, losses, or dividends from stocks. Overall, we can draw the conclusion that while these columns may contain important information, they may not be very helpful in predicting income levels
- Most people either work for a single week or for an entire year. (i.e. 52 weeks)
- Throughout the year, fewer people work part-time
- It is evident that the majority of individuals with annual incomes of at least 50,000 work for an entire year

Model Architecture and Training Algorithm:
In this project, feature selection is an essential step to improve the classification model's accuracy and efficiency. SelectKBest is a widely used feature selection method that helps identify the most relevant features from a dataset. In this project, the SelectKBest method is used to select the 15 best features from the data. However, the value of k in the SelectKBest method can be changed to test if there can be an improved performance with fewer or more features.
Since the target variable is imbalanced, with the majority of individuals earning less than $50,000, undersampling techniques are used to balance the dataset. RandomUnderSampler is used to perform undersampling, which randomly selects data from the majority class to match the number of samples in the minority class. It is observed that the RandomUnderSampler method provides better results than other undersampling techniques. Adressing the imbalanced data with oversampling techniques could also be an other approach.
Baseline models such as KNN, logistic regression, Random Forest, and SVM are implemented to identify which model performs better for this classification task. The models are trained on 80% of the data and evaluated using the remaining 20%. Evaluation metrics such as accuracy, precision, recall, and F1-score are used to assess the models' performance. By comparing the evaluation metrics, the best-performing model can be selected for further analysis and improvements.
The segmentation model is a powerful tool for dividing data into distinct groups based on certain characteristics or attributes. One of the most widely used techniques for this purpose is the k-means algorithm. In this project, we have employed the k-means algorithm to cluster the data and identify similar groups.
To choose the optimal k value for the model, we have used two popular methods - elbow and silhouette scores. The elbow method helps to identify the point of inflection, where the addition of clusters does not significantly improve the performance of the model. On the other hand, the silhouette score measures how well each data point fits within its assigned cluster, with higher scores indicating better performance.
After evaluating both methods, we have determined that k=2 has a better silhouette score of 0.58 than k=3, which has a score of 0.55. Therefore, we have proceeded with two clusters in this model.
While implementing the select k best feature selection method to improve the model is an option, it is important to note that the data used in this project has already been cleaned and label encoded. Therefore, it may not necessarily result in a significant improvement in performance.
Overall, by implementing the k-means algorithm and selecting the optimal k value using appropriate evaluation metrics, we can effectively segment the data into distinct groups and gain valuable insights for further analysis.

Evaluation Procedure and Findings:
Random Forest is a powerful algorithm for classification tasks, and in this project, it achieved decent performance with an accuracy of 85%, a precision of 83%, a recall of 87%, and an F1-score of 85%. However, to further improve the model's performance, Grid Search CV is applied to the random forest model to get the best hyperparameters. Grid Search CV is a technique that allows us to evaluate multiple combinations of hyperparameters to determine the optimal values for the model.
After applying Grid Search CV, the model's performance is further improved, achieving an accuracy of 85%, a precision of 82%, a recall of 88%, and an F1-score of 85%. This shows that tuning hyperparameters can make a significant impact on the model's performance.
It is important to note that while the improvement in performance may seem small, it is still significant, especially in scenarios where even small improvements in accuracy, precision, or recall can make a difference. Additionally, it is crucial to avoid overfitting the model when tuning hyperparameters. Overfitting occurs when the model is too complex and fits too closely to the training data, leading to poor performance on unseen data. Therefore, it is essential to use techniques like cross-validation to ensure that the model generalizes well on new data.
In conclusion, the use of Grid Search CV to tune hyperparameters can help to improve the performance of machine learning models. However, it is crucial to avoid overfitting and use proper evaluation techniques to ensure that the model generalizes well on new data.
To analyze the patterns among the clusters and reduce the computational load, a sample dataset was created. K-means clustering with 2 clusters was initially implemented, but the results were not clear enough to observe the patterns. Therefore, 3 clusters were formed, and a more detailed analysis was carried out. The features that were considered important to observe the similarities and differences among the clusters were age, class of worker, major industry code, full or part-time status, and race.
The clustering results showed that the 3 clusters had distinct characteristics. The first cluster consisted of mainly children,never married and non filer of tax having income less than 50,000. The second cluster included aged (18-60years) worker in private sector or attending high school , if married spouse are present and mostly not Hispanic. The third cluster was dominated by armed forces and unemployed individuals, also who were working less number of weeks in a year. Furthermore, it was observed that the clusters were similar in terms of citizenship and race.
Overall, the analysis provided a better understanding of the characteristics of the different clusters and highlighted the importance of considering multiple features while performing clustering.

References:
• niteshyadav3103. (2022, March 10). Customer segmentation using Kmeans, HC &amp; DBSCAN. Kaggle. Retrieved March 17, 2023, from https://www.kaggle.com/code/niteshyadav3103/customer-segmentation-using-kmeans-hc-dbscan
• Sagar, A. (2019, December 3). Customer segmentation using K means clustering. Medium. Retrieved March 17, 2023, from https://towardsdatascience.com/customer-segmentation-using-k-means-clustering-d33964f238c3
• Building a classifier from census data: District Data Labs. District Data Labs: Data Science Consulting and Corporate Training. (n.d.). Retrieved March 17, 2023, from https://www.districtdatalabs.com/building-a-classifier-from-census-data
